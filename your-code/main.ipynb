{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some web scraping exercises to practice your scraping skills using `requests` and `Beautiful Soup`.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the [response status code](https://http.cat/) for each request to ensure you have obtained the intended content.\n",
    "- Look at the HTML code in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract.\n",
    "- Check out the css selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, gathering our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Again, please remember to limit your output before submission so that your code doesn't get lost in the output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_gh = \"https://github.com/trending/developers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_gh = \"https://github.com/trending/developers\"\n",
    "response = requests.get(url_gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below (with different names):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (Á•ûÊ•ΩÂùÇË¶ö„ÄÖ)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_names_tags = soup.find_all(\n",
    "    name=\"h1\", \n",
    "    class_=\"h3 lh-condensed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trending_names_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"h3 lh-condensed\">\n",
       "<a data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"TRENDING_DEVELOPERS_PAGE\",\"click_target\":\"OWNER\",\"click_visual_representation\":\"TRENDING_DEVELOPER\",\"actor_id\":null,\"record_id\":928116,\"originating_url\":\"https://github.com/trending/developers\",\"user_id\":null}}' data-hydro-click-hmac=\"25015dfe1247ba4a6f212624be2a96d03de5b86131b98d7c372384c98be9e8c5\" href=\"/mlocati\">\n",
       "            Michele Locati\n",
       "</a> </h1>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trending_names_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n            Michele Locati\\n '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trending_names_tags[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n            Michele Locati\\n ', '\\n\\n            Han Xiao\\n ']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [name.text for name in trending_names_tags]\n",
    "names[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['            Michele Locati ', '            Han Xiao ']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_f=[]\n",
    "for name in names:\n",
    "    names_f.append(name.replace(\"\\n\",\"\"))\n",
    "names_f[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michele Locati',\n",
       " 'Han Xiao',\n",
       " 'Alon Zakai',\n",
       " 'Ha Thach',\n",
       " 'Joel Arvidsson',\n",
       " 'Major Hayden',\n",
       " 'Ahmet Alp Balkan',\n",
       " 'Don Jayamanne',\n",
       " 'Alexey Golub',\n",
       " 'Diego Muracciole',\n",
       " 'Simen Bekkhus',\n",
       " 'Dan Davison',\n",
       " 'Mo Gorhom',\n",
       " 'Jos√© Padilla',\n",
       " 'Yevgeniy Brikman',\n",
       " 'Thibault Duplessis',\n",
       " 'Nelson Osacky',\n",
       " 'Gleb Bahmutov',\n",
       " 'Soledad Galli',\n",
       " 'Gerasimos (Makis) Maropoulos',\n",
       " 'Nicolas P. Rougier',\n",
       " 'snipe',\n",
       " 'Anthony Sottile',\n",
       " 'Julius H√§rtl',\n",
       " 'Samuel Colvin']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name.strip() for name in names_f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 2 - Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_2 = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2 = \"https://github.com/trending/python?since=daily\"\n",
    "response2 = requests.get(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_names_tags = soup2.find_all(\n",
    "    name=\"h1\", \n",
    "    class_=\"h3 lh-condensed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_names_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"h3 lh-condensed\">\n",
       "<a data-hydro-click='{\"event_type\":\"explore.click\",\"payload\":{\"click_context\":\"TRENDING_REPOSITORIES_PAGE\",\"click_target\":\"REPOSITORY\",\"click_visual_representation\":\"REPOSITORY_NAME_HEADING\",\"actor_id\":null,\"record_id\":240315046,\"originating_url\":\"https://github.com/trending/python?since=daily\",\"user_id\":null}}' data-hydro-click-hmac=\"4e4ae9fe56c93d4b1c3c4d96b377380b68f4727741403591b8b5bc5ee5cc0575\" href=\"/jina-ai/jina\">\n",
       "<svg aria-hidden=\"true\" class=\"octicon octicon-repo mr-1 text-gray\" height=\"16\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"16\"><path d=\"M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z\" fill-rule=\"evenodd\"></path></svg>\n",
       "<span class=\"text-normal\">\n",
       "        jina-ai /\n",
       "</span>\n",
       "      jina\n",
       "</a> </h1>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_names_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n        jina-ai /\\n\\n      jina\\n '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_names_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n        jina-ai /\\n\\n      jina\\n ',\n",
       " '\\n\\n\\n\\n        MTK-bypass /\\n\\n      bypass_utility\\n ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = [repo.text for repo in repo_names_tags]\n",
    "repos[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jina-ai/jina',\n",
       " 'MTK-bypass/bypass_utility',\n",
       " 'ansible/awx',\n",
       " 'Asabeneh/30-Days-Of-Python',\n",
       " 'PaddlePaddle/PaddleOCR']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos_f=[]\n",
    "for repo in repos:\n",
    "    repos_f.append(repo.replace(\"\\n\",\"\").replace(\" \",\"\"))\n",
    "repos_f[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 3 - Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = \"https://en.wikipedia.org/wiki/Walt_Disney\"\n",
    "response3 = requests.get(url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(response3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tags = soup3.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img alt=\"This is a featured article. Click here for more information.\" data-file-height=\"438\" data-file-width=\"462\" decoding=\"async\" height=\"19\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/30px-Cscr-featured.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/40px-Cscr-featured.svg.png 2x\" width=\"20\"/>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tags[0].get(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'range'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-c8a355119725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#No consigo sacar la lista completa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimage_tags\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimg_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mimage_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"src\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage_tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2172\u001b[0m         \u001b[1;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2173\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m   2174\u001b[0m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2175\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'range'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "#No consigo sacar la lista completa\n",
    "image_tags= []\n",
    "for x in img_tags.range():\n",
    "    image_tags.append(img_tags[x].get(\"src\"))\n",
    "image_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tags.find_all(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otro modo de acceder:\n",
    "disney_img_tags = soup3.find_all(\n",
    "    name=\"a\", \n",
    "    class_=\"image\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"image\" href=\"/wiki/File:Walt_Disney_1946.JPG\"><img alt=\"Walt Disney 1946.JPG\" data-file-height=\"675\" data-file-width=\"450\" decoding=\"async\" height=\"330\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/330px-Walt_Disney_1946.JPG 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/440px-Walt_Disney_1946.JPG 2x\" width=\"220\"/></a>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disney_img_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deber√≠a salir el src...!!\n",
    "disney_img_tags[0].get(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 =\"https://en.wikipedia.org/wiki/Python\"\n",
    "response4 = requests.get(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup4 = BeautifulSoup(response4.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cojo todo el cuerpo de la web\n",
    "python_link_tags = soup4.find_all(\n",
    "    name=\"div\", \n",
    "    class_=\"mw-body-content\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nPython - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPython\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\n\\nJump to navigation\\nJump to search\\n\\n\\n\\nLook up Python\\xa0or python in Wiktionary, the free dictionary.\\n\\nPython may refer to:\\n\\nPythons or Pythonidae, a family of nonvenomous snakes found in Africa, Asia, and Australia\\nPython (genus), a genus of nonvenomous Pythonidae found in Africa and Asia\\nContents\\n\\n1 Computing\\n2 People\\n3 Roller coasters\\n4 Vehicles\\n5 Weaponry\\n6 Other uses\\n7 See also\\n\\n\\n\\nComputing[edit]\\nPython (programming language)\\nPython, a native code compiler for CMU Common Lisp\\nPython, the internal project name for the PERQ 3 computer workstation\\nPeople[edit]\\nPython of Aenus (4th-century BCE), student of Plato\\nPython (painter), (ca. 360-320 BCE) vase painter in Poseidonia\\nPython of Byzantium, orator, diplomat of Philip II of Macedon\\nPython of Catana, poet who accompanied Alexander the Great\\nPython Anghelo (1954‚Äì2014) Romanian graphic artist\\nRoller coasters[edit]\\nPython (Efteling), a roller coaster in the Netherlands\\nPython (Busch Gardens Tampa Bay), a defunct roller coaster\\nPython (Coney Island, Cincinnati, Ohio), a steel roller coaster\\nVehicles[edit]\\nPython (automobile maker), an Australian car company\\nPython (Ford prototype), a Ford prototype sports car\\nWeaponry[edit]\\nPython (missile), a series of Israeli air-to-air missiles\\nPython (nuclear primary), a gas-boosted fission primary used in thermonuclear weapons\\nColt Python, a revolver\\nOther uses[edit]\\nPYTHON, a British nuclear war contingency plan\\nPython (film), a 2000 horror film by Richard Clabaugh\\nPython (mythology), a mythical serpent\\nMonty Python or the Pythons, a British comedy group\\nPython (Monty) Pictures, a company owned by the troupe\\'s surviving members\\nSee also[edit]\\nCython, a programming language superset of Python\\nPyton, a Norwegian magazine\\nPithon\\n\\n\\n\\n \\n Disambiguation page providing links to topics that could be referred to by the same search termThis disambiguation page lists  articles associated with the title Python.  If an internal link led you here, you may wish to change the link to point directly to the intended article. \\n\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Python&oldid=997582414\"\\nCategories: Disambiguation pagesHuman name disambiguation pagesDisambiguation pages with given-name-holder listsHidden categories: Disambiguation pages with short descriptionsShort description is different from WikidataAll article disambiguation pagesAll disambiguation pagesAnimal common name disambiguation pages\\n\\n\\n\\n\\n\\n\\nNavigation menu\\n\\n\\n\\n\\nPersonal tools\\n\\n\\nNot logged inTalkContributionsCreate accountLog in\\n\\n\\n\\n\\n\\n\\nNamespaces\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\n\\nVariants\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nViews\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\nMore\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\nContribute\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\nTools\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item\\n\\n\\n\\n\\n\\nPrint/export\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\nIn other projects\\n\\n\\nWikimedia Commons\\n\\n\\n\\n\\n\\nLanguages\\n\\n\\nAfrikaansAlemannischÿßŸÑÿπÿ±ÿ®Ÿäÿ©Az…ôrbaycanca‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ–ë–µ–ª–∞—Ä—É—Å–∫–∞—è–ë—ä–ª–≥–∞—Ä—Å–∫–∏ƒåe≈°tinaDanskDeutschEsperantoEuskaraŸÅÿßÿ±ÿ≥€åFran√ßaisÌïúÍµ≠Ïñ¥HrvatskiIdoBahasa IndonesiaInterlingua√çslenskaItaliano◊¢◊ë◊®◊ô◊™·É•·Éê·É†·Éó·É£·Éö·ÉòKongoLatinaL√´tzebuergeschMagyar‡§Æ‡§∞‡§æ‡§†‡•ÄNederlandsÊó•Êú¨Ë™ûNorsk bokm√•lPolskiPortugu√™s–†—É—Å—Å–∫–∏–πSlovenƒçina–°—Ä–ø—Å–∫–∏ / srpskiSrpskohrvatski / —Å—Ä–ø—Å–∫–æ—Ö—Ä–≤–∞—Ç—Å–∫–∏SuomiSvenska‡πÑ‡∏ó‡∏¢T√ºrk√ße–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ÿßÿ±ÿØŸàTi·∫øng Vi·ªát‰∏≠Êñá\\nEdit links\\n\\n\\n\\n\\n\\n\\n This page was last edited on 1 January 2021, at 06:46\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License;\\nadditional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia¬Æ is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nMobile view\\nDevelopers\\nStatistics\\nCookie statement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_soup4 = soup4.get_text()\n",
    "text_soup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No lo consigo, volver√© m√°s adelante...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 5 - Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url5 = \"http://uscode.house.gov/download/download.shtml\"\n",
    "response5 = requests.get(url5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup5 = BeautifulSoup(response5.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_code_tags = soup5.find_all(\n",
    "    name=\"div\", \n",
    "    class_=\"usctitlechanged\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of changed Titles\n",
    "len(changed_code_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n          Title 6 - Domestic Security\\n\\n        ',\n",
       " '\\n\\n          Title 10 - Armed Forces Ÿ≠\\n',\n",
       " '\\n\\n          Title 15 - Commerce and Trade\\n\\n        ',\n",
       " '\\n\\n          Title 20 - Education\\n\\n        ',\n",
       " '\\n\\n          Title 28 - Judiciary and Judicial Procedure Ÿ≠\\n',\n",
       " '\\n\\n          Title 31 - Money and Finance Ÿ≠\\n',\n",
       " '\\n\\n          Title 33 - Navigation and Navigable Waters\\n\\n        ',\n",
       " '\\n\\n          Title 36 - Patriotic and National Observances, Ceremonies, and Organizations Ÿ≠\\n',\n",
       " '\\n\\n          Title 37 - Pay and Allowances of the Uniformed Services Ÿ≠\\n',\n",
       " \"\\n\\n          Title 38 - Veterans' Benefits Ÿ≠\\n\",\n",
       " '\\n\\n          Title 40 - Public Buildings, Property, and Works Ÿ≠\\n',\n",
       " '\\n\\n          Title 46 - Shipping Ÿ≠\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed_code_text = [name.text for name in changed_code_tags]\n",
    "changed_code_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['          Title 6 - Domestic Security        ',\n",
       " '          Title 10 - Armed Forces Ÿ≠',\n",
       " '          Title 15 - Commerce and Trade        ',\n",
       " '          Title 20 - Education        ',\n",
       " '          Title 28 - Judiciary and Judicial Procedure Ÿ≠',\n",
       " '          Title 31 - Money and Finance Ÿ≠',\n",
       " '          Title 33 - Navigation and Navigable Waters        ',\n",
       " '          Title 36 - Patriotic and National Observances, Ceremonies, and Organizations Ÿ≠',\n",
       " '          Title 37 - Pay and Allowances of the Uniformed Services Ÿ≠',\n",
       " \"          Title 38 - Veterans' Benefits Ÿ≠\",\n",
       " '          Title 40 - Public Buildings, Property, and Works Ÿ≠',\n",
       " '          Title 46 - Shipping Ÿ≠']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed_code_list=[]\n",
    "for name in changed_code_text:\n",
    "    changed_code_list.append(name.replace(\"\\n\",\"\"))\n",
    "changed_code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 6 - Domestic Security',\n",
       " 'Title 10 - Armed Forces Ÿ≠',\n",
       " 'Title 15 - Commerce and Trade',\n",
       " 'Title 20 - Education',\n",
       " 'Title 28 - Judiciary and Judicial Procedure Ÿ≠',\n",
       " 'Title 31 - Money and Finance Ÿ≠',\n",
       " 'Title 33 - Navigation and Navigable Waters',\n",
       " 'Title 36 - Patriotic and National Observances, Ceremonies, and Organizations Ÿ≠',\n",
       " 'Title 37 - Pay and Allowances of the Uniformed Services Ÿ≠',\n",
       " \"Title 38 - Veterans' Benefits Ÿ≠\",\n",
       " 'Title 40 - Public Buildings, Property, and Works Ÿ≠',\n",
       " 'Title 46 - Shipping Ÿ≠']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List of changed Titles\n",
    "[name.strip() for name in changed_code_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 6 - A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url6 = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url6 = \"https://www.fbi.gov/wanted/topten\"\n",
    "response6 = requests.get(url6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup6 = BeautifulSoup(response6.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_name_tags = soup6.find_all(\n",
    "    name=\"h3\", \n",
    "    class_=\"title\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wanted_name_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3 class=\"title\">\n",
       "<a href=\"https://www.fbi.gov/wanted/topten/arnoldo-jimenez\">ARNOLDO JIMENEZ</a>\n",
       "</h3>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_name_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nARNOLDO JIMENEZ\\n',\n",
       " '\\nJASON DEREK BROWN\\n',\n",
       " '\\nALEXIS FLORES\\n',\n",
       " '\\nJOSE RODOLFO VILLARREAL-HERNANDEZ\\n',\n",
       " '\\nEUGENE PALMER\\n',\n",
       " '\\nRAFAEL CARO-QUINTERO\\n',\n",
       " '\\nROBERT WILLIAM FISHER\\n',\n",
       " '\\nBHADRESHKUMAR CHETANBHAI PATEL\\n',\n",
       " '\\nALEJANDRO ROSALES CASTILLO\\n',\n",
       " '\\nYASER ABDEL SAID\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_names = [name.text for name in wanted_name_tags]\n",
    "wanted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted_names_list=[]\n",
    "for name in wanted_names:\n",
    "    wanted_names_list.append(name.replace(\"\\n\",\"\"))\n",
    "wanted_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url7 = \"https://www.fbi.gov/wanted/topten\"\n",
    "response7 = requests.get(url7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup7 = BeautifulSoup(response7.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup7.find_all(\"div\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_number_tags = soup7.find_all(\n",
    "    name=\"div\", \n",
    "    class_=\"central-featured\",\n",
    "    id_=\"primary links\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 8 - A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url8 = \"https://data.gov.uk\"\n",
    "response8 = requests.get(url8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup8 = BeautifulSoup(response8.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_tags = soup8.find_all(\n",
    "    name=\"a\", \n",
    "    class_=\"govuk-link\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(database_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"govuk-link\" href=\"/cookies\">cookies to collect information</a>,\n",
       " <a class=\"govuk-link\" data-module=\"track-click\" data-track-action=\"Cookie banner settings clicked from confirmation\" data-track-category=\"cookieBanner\" href=\"/cookies\">change your cookie settings</a>,\n",
       " <a class=\"govuk-link\" href=\"http://www.smartsurvey.co.uk/s/3SEXD/\">feedback</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Business+and+economy\">Business and economy</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Crime+and+justice\">Crime and justice</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Defence\">Defence</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Education\">Education</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Environment\">Environment</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Government\">Government</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Government+spending\">Government spending</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Health\">Health</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Mapping\">Mapping</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Society\">Society</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Towns+and+cities\">Towns and cities</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?filters%5Btopic%5D=Transport\">Transport</a>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_names = [name.text for name in database_tags]\n",
    "database_names[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping up the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(city):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 14 - Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you limit your output? Thank you! üôÇ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
